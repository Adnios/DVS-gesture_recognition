R3DCNN,C3D代码复现

论文：

**C3D: Generic Features for Video Analysis**







**Learning to Estimate 3D Hand Pose from Single RGB Images**

https://lmb.informatik.uni-freiburg.de/projects/hand3d/

从2D彩色图像进行3D hand pose estimation，手语识别

**网络结构论文摘要**

之前大部分的论文都是基于深度图像的，这篇论文提出了一个从RGB图像中估计三维节点的方法，并提出了一个大规模的3D手部姿态的RGB数据集。

 **整体网络结构：**

![img](https://images2017.cnblogs.com/blog/716743/201711/716743-20171103221330591-833650674.png)

**论文框架**

论文一共用了三个网络结构进行手部节点的位置预测，首先用一个网络提取手部区域，并重新resize手部区域的大小，然后用一个网络定位2D手部关节点的位置，在根据2D的节点位置和先验知识恢复出3D节点位置。

**相关工作**

论文主要借鉴了 2D Human Pose Estimation, 3D Human Pose Estimation 和 Hand Pose Estimation的主要方法， 感觉之后视野应该要放开阔一些，熟悉 Pose Estimation 的各种方法， 重点掌握尝试并应用 Hand Pose Estimation 的各种方法。

现在主要存在的问题有两个：一个是还是依赖于深度图像，另一个是和数据集相关度比较大， 对数据集相关的手势预测的较为准确。

**Hand pose representation**

这一部分主要是为了解决大小手，坐标等问题。

第一个方面是手的scale 问题，即使是分割出手的部分再进行resize，手的大小也会对预测结果产生影响，所以论文里面利用食指的第一根手骨的长度对手节点位置进行正则化。其次，绝对坐标系的使用会给预测带来困难，所以论文里采用了相对坐标系，以手掌节点的位置作为坐标原点对各个节点坐标进行平移。

**HandSegNet**

HandSegNet， 输入256x256x3, 输出256x256x1的一个hand mask。

将2D手部检测问题转化为一个分割问题，最后得到了Hand mask，再对手部区域提取和正则化 

实现细节：

![img](https://images2017.cnblogs.com/blog/716743/201711/716743-20171103215129982-886725569.png) 

网络结构，(Conv+ReLu)+MaxPool+Bilinear Upsampling

Loss 函数， standrad softmax + cross entropy loss

Learning rate 初始化为 1e-5，2W次迭代后变为1e-6，3W次迭代后变为1e-7

做了简单的数据増广，random color hue augmentation of 0:1 

**PoseNet**

用来计算手的21个keypoint， 输入256x256x3，输出32x32x21， 即21张不同keypoint的score map

这部分是对手部的每一个节点预测出一个二维的热图。也是采用 encoder-decoder的结构。

实现细节：

![img](https://images2017.cnblogs.com/blog/716743/201711/716743-20171103215552779-1391624653.png)

网络结构，(Conv+ReLu)+MaxPool，最后的预测利用了17,24和31层的feature map

Loss 函数，L2 Loss

需要注意的是对于Ground Truth的处理，利用了均值为关节点位置，方差为25个像素的高斯分布。**而对于不可见的节点，所有的概率值设为0。**

对于数据crop这部分，采用了两个方法进行，一个是对bounding box的中心加了0均值，方差为10的高斯噪声，另一个是对节点热图加了0均值，方差为1.5的高斯噪声。训练过程初始学习率1e-4，每一万次缩小十倍。

**PosePrior**

有两个子stream，每个的网络结构除了最后一层不一样其他都一样，输入32x32x21，输出两个层。一个是正则化的手的坐标，以手掌的点为原点，且长度进行了normalize， 即维度为21x3。另一个是相对于实际图片的空间的变换关系，即维度为3

两个任务，一个是预测节点位置信息，一个是估计视角角度，这两个任务用了接近相同的网络框架。再将预测结果进行融合得到最后的坐标。

实现细节：

![img](https://images2017.cnblogs.com/blog/716743/201711/716743-20171103221002107-872302928.png)

这部分的网络结构很简单。

Loss 函数， ![img](https://images2017.cnblogs.com/blog/716743/201711/716743-20171103221148091-942150800.png)+![img](https://images2017.cnblogs.com/blog/716743/201711/716743-20171103221155545-1952542594.png)

